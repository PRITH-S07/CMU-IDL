{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28297c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prachipatil/Desktop/CMU Courses/IDL/Project/CMU-IDL/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from models.discriminator import CLIPSVMDiscriminator\n",
    "from adversarial_models.CLIP_pgd import CLIPPGDAttack\n",
    "from adversarial_models.CLIP_patch_pgd import CLIPPatchPGDAttack\n",
    "import torch\n",
    "import torchvision\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "612ac0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = \"subset/sources\"\n",
    "processed_dir = \"subset/processed\"\n",
    "pgd_dir = \"subset/pgd\"\n",
    "pgd_patch_dir = \"subset/pgd_patch\"\n",
    "pgd_attn_dir = \"subset/pgd_attn\"\n",
    "\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "os.makedirs(pgd_dir, exist_ok=True)\n",
    "os.makedirs(pgd_patch_dir, exist_ok=True)\n",
    "os.makedirs(pgd_attn_dir, exist_ok=True)\n",
    "\n",
    "model_path = \"subset/CLIP_discriminator.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61c02a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model: CLIPSVMDiscriminator = torch.load(\n",
    "    model_path, weights_only=False, map_location=torch.device(\"cpu\")\n",
    ")\n",
    "model.device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3474ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = model.model\n",
    "processor = model.processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe874f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_images = [\n",
    "    torchvision.io.read_image(os.path.join(source_dir, filepath))\n",
    "    for filepath in os.listdir(source_dir)\n",
    "]\n",
    "len(src_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "956df1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocesses and save images to processed_dir\n",
    "paths = os.listdir(source_dir)\n",
    "images = processor(images=src_images, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "\n",
    "\n",
    "for i, image in enumerate(images):\n",
    "    torchvision.utils.save_image(image, os.path.join(processed_dir, paths[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b10339bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.242676\n",
      "Loss: -13.095894\n",
      "Loss: -23.325668\n",
      "Loss: -27.238659\n",
      "Loss: -31.671082\n",
      "Loss: -33.963562\n",
      "Loss: -42.636475\n",
      "Loss: -43.300907\n",
      "Loss: -41.078064\n",
      "Loss: -45.779037\n",
      "Evaluating subset/poster/e_8\n",
      "Accuracy: 0.0000\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "AUC: nan\n",
      "mAP: 1.0000\n",
      "Probabilities: [1. 1.]\n",
      "Predictions: [1 1] (1 = real, 0 = fake)\n",
      "Raw scores: [2.20022039 4.14677584]\n",
      "Files: ['OpenJourney_9751.jpg', 'dalle_148.jpg']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prachipatil/Desktop/CMU Courses/IDL/Project/CMU-IDL/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/prachipatil/Desktop/CMU Courses/IDL/Project/CMU-IDL/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/prachipatil/Desktop/CMU Courses/IDL/Project/CMU-IDL/.venv/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "attack = CLIPPGDAttack(discriminator, model.svm, eps=8/255, random_start=True)\n",
    "\n",
    "inpaths = [\n",
    "    \"subset/poster/srcs/dalle_148.jpg\", \"subset/poster/srcs/OpenJourney_9751.jpg\"\n",
    "]\n",
    "outpaths =  [\n",
    "    \"subset/poster/e_8/dalle_148.jpg\", \"subset/poster/e_8/OpenJourney_9751.jpg\"\n",
    "]\n",
    "\n",
    "images = [\n",
    "    torchvision.io.read_image(filepath)\n",
    "    for filepath in inpaths]\n",
    "resize_transform = torchvision.transforms.Resize((224, 224))\n",
    "images = torch.stack(images).to(discriminator.device)\n",
    "images = resize_transform(images)\n",
    "images = images.float() / 255.0  # Normalize to [0, 1]\n",
    "images = resize_transform(images)\n",
    "images = torch.FloatTensor(images)\n",
    "# images = processor(images=images, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "target_label = torch.tensor([1]*len(outpaths)).to(discriminator.device)\n",
    "\n",
    "# # Generate adversarial example\n",
    "adversarial_image = attack(images, target_label)\n",
    "\n",
    "for i in range(adversarial_image.shape[0]):\n",
    "    # Get the image and filepath for the current index\n",
    "    image = adversarial_image[i]\n",
    "    filepath = outpaths[i]\n",
    "\n",
    "    # Save the image using torchvision.utils.save_image\n",
    "    torchvision.utils.save_image(image, filepath)\n",
    "\n",
    "evaluate_dir(\"subset/poster/e_8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cc3011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adversarial_models.CLIP_pgd import preprocess_image\n",
    "processed_custom = \"subset/processed_custom\"\n",
    "os.makedirs(processed_custom, exist_ok=True)\n",
    "\n",
    "transform = torchvision.transforms.Resize((224, 224))\n",
    "# preprocess all images in source_dir and save to processed_custom \n",
    "for filepath in os.listdir(source_dir):\n",
    "    image = preprocess_image(os.path.join(source_dir, filepath), transform)\n",
    "    torchvision.utils.save_image(image, os.path.join(processed_dir, filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "41341b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.339697\n",
      "Loss: 2.660016\n",
      "Loss: 0.863431\n",
      "Loss: 0.114115\n",
      "Loss: -0.916858\n",
      "Loss: -2.070945\n",
      "Loss: -2.232011\n",
      "Loss: -0.827594\n",
      "Loss: -3.275060\n",
      "Loss: -3.728092\n",
      "Loss: -4.641870\n",
      "Loss: -2.827746\n",
      "Loss: -2.181078\n",
      "Loss: -5.026191\n",
      "Loss: -4.897472\n",
      "Loss: -5.888506\n",
      "Loss: -3.707053\n",
      "Loss: -3.516158\n",
      "Loss: -5.323720\n",
      "Loss: -6.202042\n",
      "Image 0: Selected 3 patches out of 3 requested\n",
      "Image 1: Selected 3 patches out of 3 requested\n"
     ]
    }
   ],
   "source": [
    "from adversarial_models.CLIP_pgd import preprocess_image\n",
    "\n",
    "# PGD Attack with patch\n",
    "pgd_patch_dir = \"subset/poster/patch\"\n",
    "os.makedirs(pgd_patch_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(pgd_patch_dir, \"boxes\"), exist_ok=True)\n",
    "\n",
    "\n",
    "patch_attack = CLIPPatchPGDAttack(discriminator, model.svm, patch_size=32, steps=20, num_patches=3, random_start=True)\n",
    "\n",
    "inpaths = [\n",
    "    \"subset/poster/srcs/dalle_148.jpg\", \"subset/poster/srcs/OpenJourney_9751.jpg\"\n",
    "]\n",
    "patch_paths =  [\n",
    "    \"subset/poster/patch/dalle_148.jpg\", \"subset/poster/patch/OpenJourney_9751.jpg\"\n",
    "]\n",
    "\n",
    "box_paths =  [\n",
    "    \"subset/poster/patch/boxes_dalle_148.jpg\", \"subset/poster/patch/boxes_OpenJourney_9751.jpg\"\n",
    "]\n",
    "\n",
    "images = [\n",
    "    torchvision.io.read_image(filepath)\n",
    "    for filepath in inpaths]\n",
    "resize_transform = torchvision.transforms.Resize((224, 224))\n",
    "images = torch.stack(images).to(discriminator.device)\n",
    "images = resize_transform(images)\n",
    "images = images.float() / 255.0  # Normalize to [0, 1]\n",
    "images = resize_transform(images)\n",
    "images = torch.FloatTensor(images)\n",
    "\n",
    "# # Generate adversarial example\n",
    "adversarial_image, boxes = patch_attack(images, target_label)\n",
    "\n",
    "for i in range(adversarial_image.shape[0]):\n",
    "    # Get the image and filepath for the current index\n",
    "    image = adversarial_image[i]\n",
    "    filepath = patch_paths[i]\n",
    "\n",
    "    # Save the image using torchvision.utils.save_image\n",
    "    torchvision.utils.save_image(image, filepath)\n",
    "    torchvision.utils.save_image(boxes[i], box_paths[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e8f68797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating subset/poster/patch\n",
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n",
      "AUC: nan\n",
      "mAP: 1.0000\n",
      "Probabilities: [3.29749077e-05 2.03776594e-01 1.51309128e-04 2.58396320e-01]\n",
      "Predictions: [0 0 0 0] (1 = real, 0 = fake)\n",
      "Raw scores: [-1.98646241 -0.30693305 -1.7007729  -0.24907872]\n",
      "Files: ['boxes_OpenJourney_9751.jpg', 'boxes_dalle_148.jpg', 'OpenJourney_9751.jpg', 'dalle_148.jpg', 'boxes']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prachipatil/Desktop/CMU Courses/IDL/Project/CMU-IDL/.venv/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "evaluate_dir(\"subset/poster/patch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c49a6003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dir(dirname):\n",
    "    print(f\"Evaluating {dirname}\")\n",
    "    images = [\n",
    "        torchvision.io.read_image(os.path.join(dirname, filepath))\n",
    "        for filepath in os.listdir(dirname)\n",
    "        if filepath.endswith(\".png\") or filepath.endswith(\".jpg\")\n",
    "    ]\n",
    "    labels = [0] * len(images)\n",
    "    images = torch.stack(images)\n",
    "    embeddings = model.run_clip(images)\n",
    "    metrics = model.evaluate(embeddings, labels)\n",
    "    raw_scores = model.svm.decision_function(embeddings)\n",
    "    print(\"Probabilities:\", metrics[\"probs\"])\n",
    "    print(\"Predictions:\", metrics[\"preds\"], \"(1 = real, 0 = fake)\")\n",
    "    print(\"Raw scores:\", raw_scores)\n",
    "    print(\"Files:\", os.listdir(dirname))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3e5dc833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating subset/pgd\n",
      "Accuracy: 0.4000\n",
      "Precision: 1.0000\n",
      "Recall: 0.4000\n",
      "F1 Score: 0.5714\n",
      "AUC: nan\n",
      "mAP: 1.0000\n",
      "Probabilities: [0.08024108 0.37689641 0.87471683 0.99999819 0.99999994]\n",
      "Predictions: [0 0 1 1 1] (1 = real, 0 = fake)\n",
      "Raw scores: [-0.50873735 -0.14664816  0.3127578   1.29143301  1.60828624]\n",
      "Files: ['openjourney_v4_37056.jpg', 'stable_diff_9751.jpg', 'stable_diff_82434.jpeg', 'titan_83700.jpg', 'OpenJourney_9751.jpg', 'dalle_148.jpg']\n",
      "\n",
      "Evaluating subset/pgd_patch_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prachipatil/Desktop/CMU Courses/IDL/Project/CMU-IDL/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/prachipatil/Desktop/CMU Courses/IDL/Project/CMU-IDL/.venv/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects a non-empty TensorList",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# evaluate_dir(source_dir)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m evaluate_dir(pgd_dir)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mevaluate_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpgd_patch_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[84], line 9\u001b[0m, in \u001b[0;36mevaluate_dir\u001b[0;34m(dirname)\u001b[0m\n\u001b[1;32m      3\u001b[0m images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m     torchvision\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mread_image(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dirname, filepath))\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filepath \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(dirname)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filepath\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m filepath\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m ]\n\u001b[1;32m      8\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(images)\n\u001b[0;32m----> 9\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mrun_clip(images)\n\u001b[1;32m     11\u001b[0m metrics \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(embeddings, labels)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"
     ]
    }
   ],
   "source": [
    "# evaluate_dir(source_dir)\n",
    "evaluate_dir(pgd_dir)\n",
    "evaluate_dir(pgd_patch_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "3359768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "atack = CLIPPGDAttack(discriminator, model.svm, steps=3, eps=4/255, random_start=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "224e2e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/data/dalle/dalle_73262.jpg\n",
      "Adversarial example\n",
      "Loss: 6.467362\n",
      "Loss: -3.662686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -8.579271\n",
      "images/data/dalle/dalle_37748.jpg\n",
      "Adversarial example\n",
      "Loss: 6.296868\n",
      "Loss: 1.954516\n",
      "Loss: -1.314785\n",
      "images/data/dalle/dalle_20219.jpg\n",
      "Adversarial example\n",
      "Loss: 7.210421\n",
      "Loss: 1.403381\n",
      "Loss: -2.303245\n",
      "images/data/dalle/dalle_37887.jpg\n",
      "Adversarial example\n",
      "Loss: 4.587760\n",
      "Loss: 0.490902\n",
      "Loss: -2.080090\n",
      "images/data/dalle/dalle_48727.jpg\n",
      "Adversarial example\n",
      "Loss: 3.934434\n",
      "Loss: 0.392909\n",
      "Loss: -2.239207\n",
      "images/data/dalle/dalle_84165.jpg\n",
      "Adversarial example\n",
      "Loss: 2.428377\n",
      "Loss: -4.887422\n",
      "Loss: -8.634383\n",
      "images/data/dalle/dalle_19084.jpg\n",
      "Adversarial example\n",
      "Loss: 4.428191\n",
      "Loss: 1.979040\n",
      "Loss: 0.275106\n",
      "images/data/dalle/dalle_58818.jpg\n",
      "Adversarial example\n",
      "Loss: 6.389173\n",
      "Loss: 0.989652\n",
      "Loss: 0.439508\n",
      "images/data/dalle/dalle_50071.jpg\n",
      "Adversarial example\n",
      "Loss: 7.999412\n",
      "Loss: -0.929499\n",
      "Loss: -5.756628\n",
      "images/data/dalle/dalle_42853.jpg\n",
      "Adversarial example\n",
      "Loss: 4.752370\n",
      "Loss: -3.638960\n",
      "Loss: -8.515268\n",
      "images/data/dalle/dalle_90584.jpg\n",
      "Adversarial example\n",
      "Loss: 4.303026\n",
      "Loss: -2.176748\n",
      "Loss: -6.819726\n",
      "images/data/dalle/dalle_3651.jpg\n",
      "Adversarial example\n",
      "Loss: 8.066465\n",
      "Loss: 5.305557\n",
      "Loss: 1.129676\n",
      "images/data/dalle/dalle_100043.jpg\n",
      "Adversarial example\n",
      "Loss: 6.708874\n",
      "Loss: 0.025563\n",
      "Loss: -4.168192\n",
      "images/data/dalle/dalle_79910.jpg\n",
      "Adversarial example\n",
      "Loss: 0.609923\n",
      "Loss: -6.825180\n",
      "Loss: -11.474769\n",
      "images/data/dalle/dalle_100632.jpg\n",
      "Adversarial example\n",
      "Loss: 10.386426\n",
      "Loss: 4.425368\n",
      "Loss: 0.946659\n",
      "images/data/dalle/dalle_12940.jpg\n",
      "Adversarial example\n",
      "Loss: 1.066784\n",
      "Loss: -5.828559\n",
      "Loss: -8.968344\n",
      "images/data/dalle/dalle_21539.jpg\n",
      "Adversarial example\n",
      "Loss: 0.623380\n",
      "Loss: -4.721379\n",
      "Loss: -7.678077\n",
      "images/data/dalle/dalle_34949.jpg\n",
      "Adversarial example\n",
      "Loss: 5.447825\n",
      "Loss: -0.047156\n",
      "Loss: -3.817461\n",
      "images/data/dalle/dalle_20566.jpg\n",
      "Adversarial example\n",
      "Loss: 14.964597\n",
      "Loss: 8.723815\n",
      "Loss: 4.339535\n",
      "images/data/dalle/dalle_79903.jpg\n",
      "Adversarial example\n",
      "Loss: 5.910904\n",
      "Loss: 3.986682\n",
      "Loss: 2.609904\n",
      "images/data/real/71149.jpg\n",
      "Real example\n",
      "images/data/real/66048.jpg\n",
      "Real example\n",
      "images/data/real/37097.jpg\n",
      "Real example\n",
      "images/data/real/86120.jpg\n",
      "Real example\n",
      "images/data/real/15812.jpg\n",
      "Real example\n",
      "images/data/real/62538.jpg\n",
      "Real example\n",
      "images/data/real/60694.jpg\n",
      "Real example\n",
      "images/data/real/14960.jpg\n",
      "Real example\n",
      "images/data/real/64589.jpg\n",
      "Real example\n",
      "images/data/real/47215.jpg\n",
      "Real example\n",
      "images/data/real/51732.jpg\n",
      "Real example\n",
      "images/data/real/26363.jpg\n",
      "Real example\n",
      "images/data/real/37534.jpg\n",
      "Real example\n",
      "images/data/real/18302.jpg\n",
      "Real example\n",
      "images/data/real/9504.jpg\n",
      "Real example\n",
      "images/data/real/96357.jpg\n",
      "Real example\n",
      "images/data/real/5829.jpg\n",
      "Real example\n",
      "images/data/real/34841.jpg\n",
      "Real example\n",
      "images/data/real/12801.jpg\n",
      "Real example\n",
      "images/data/real/10130.jpg\n",
      "Real example\n",
      "images/data/titan/titan_13782.jpg\n",
      "Adversarial example\n",
      "Loss: 6.990449\n",
      "Loss: 1.144169\n",
      "Loss: -3.191258\n",
      "images/data/titan/titan_82366.jpg\n",
      "Adversarial example\n",
      "Loss: 15.276445\n",
      "Loss: 9.414387\n",
      "Loss: 2.186018\n",
      "images/data/titan/titan_6078.jpg\n",
      "Adversarial example\n",
      "Loss: 5.585823\n",
      "Loss: -4.724041\n",
      "Loss: -8.336787\n",
      "images/data/titan/titan_77278.jpg\n",
      "Adversarial example\n",
      "Loss: 10.891291\n",
      "Loss: 4.535161\n",
      "Loss: -1.361673\n",
      "images/data/titan/titan_81823.jpg\n",
      "Adversarial example\n",
      "Loss: 17.447407\n",
      "Loss: 8.957441\n",
      "Loss: 3.132206\n",
      "images/data/titan/titan_20092.jpg\n",
      "Adversarial example\n",
      "Loss: 12.612817\n",
      "Loss: 6.803777\n",
      "Loss: 2.564006\n",
      "images/data/titan/titan_3381.jpg\n",
      "Adversarial example\n",
      "Loss: 10.182925\n",
      "Loss: 3.393796\n",
      "Loss: -2.063401\n",
      "images/data/titan/titan_11691.jpg\n",
      "Adversarial example\n",
      "Loss: 17.166018\n",
      "Loss: 10.408494\n",
      "Loss: 5.573655\n",
      "images/data/titan/titan_10338.jpg\n",
      "Adversarial example\n",
      "Loss: 3.344923\n",
      "Loss: -2.598517\n",
      "Loss: -6.443959\n",
      "images/data/titan/titan_7551.jpg\n",
      "Adversarial example\n",
      "Loss: 11.318770\n",
      "Loss: 4.169816\n",
      "Loss: -2.463675\n",
      "images/data/titan/titan_77781.jpg\n",
      "Adversarial example\n",
      "Loss: 10.373896\n",
      "Loss: 2.330090\n",
      "Loss: -1.998659\n",
      "images/data/titan/titan_56957.jpg\n",
      "Adversarial example\n",
      "Loss: 10.141197\n",
      "Loss: 3.246066\n",
      "Loss: -3.214799\n",
      "images/data/titan/titan_18955.jpg\n",
      "Adversarial example\n",
      "Loss: 6.968610\n",
      "Loss: 0.623694\n",
      "Loss: -1.305395\n",
      "images/data/titan/titan_75805.jpg\n",
      "Adversarial example\n",
      "Loss: 13.610871\n",
      "Loss: 7.154003\n",
      "Loss: 2.326788\n",
      "images/data/titan/titan_10310.jpg\n",
      "Adversarial example\n",
      "Loss: 7.459260\n",
      "Loss: -0.174606\n",
      "Loss: -4.575146\n",
      "images/data/titan/titan_47671.jpg\n",
      "Adversarial example\n",
      "Loss: 7.007777\n",
      "Loss: 0.253308\n",
      "Loss: -3.900948\n",
      "images/data/titan/titan_59531.jpg\n",
      "Adversarial example\n",
      "Loss: 9.675092\n",
      "Loss: 1.651010\n",
      "Loss: -3.214340\n",
      "images/data/titan/titan_26216.jpg\n",
      "Adversarial example\n",
      "Loss: 12.496041\n",
      "Loss: 6.611542\n",
      "Loss: 2.523202\n",
      "images/data/titan/titan_78436.jpg\n",
      "Adversarial example\n",
      "Loss: 15.704996\n",
      "Loss: 10.134918\n",
      "Loss: 6.218907\n",
      "images/data/titan/titan_77749.jpg\n",
      "Adversarial example\n",
      "Loss: 11.651028\n",
      "Loss: 2.004183\n",
      "Loss: -4.826655\n"
     ]
    }
   ],
   "source": [
    "# Add this after the attack but before using the results\n",
    "import random\n",
    "# walk through every image in images directory, run the PGD attack on it, and save the result to pgd_dir\n",
    "# attack = CLIPPatchPGDAttack(discriminator, model.svm, eps=0.3, steps=20, patch_size=32, num_patches=5, random_start=True)\n",
    "attack = CLIPPGDAttack(discriminator, model.svm, steps=3, eps=4/255, random_start=True)\n",
    "preds = []\n",
    "labels = []\n",
    "resize_transform = torchvision.transforms.Resize((224, 224))\n",
    "for dirpath in [\"images/data/dalle\", \"images/data/real\", \"images/data/titan\"]:\n",
    "    filenames = os.listdir(dirpath)\n",
    "    random.shuffle(filenames)\n",
    "    for filename in filenames[:20]:\n",
    "        try:\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            if filepath.endswith(\".png\") or filepath.endswith(\".jpg\"):\n",
    "                # Process the image\n",
    "                print(filepath)\n",
    "                if \"real\" not in filepath:\n",
    "                    print(\"Adversarial example\")\n",
    "                    image = torchvision.io.read_image(filepath)\n",
    "                    image = processor(images=image, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "                    target_label = torch.tensor([1]).to(discriminator.device)\n",
    "\n",
    "                    # Generate adversarial example\n",
    "                    adversarial_image = attack(image, target_label)\n",
    "                    \n",
    "             \n",
    "\n",
    "                    embedding = model.run_clip(adversarial_image).reshape(1, -1)\n",
    "\n",
    "                    pred, prob = model.predict_from_embeddings(embedding)\n",
    "\n",
    "                    preds.append(pred.item())\n",
    "                    labels.append(0)\n",
    "                else:\n",
    "                    print(\"Real example\")\n",
    "                    preds.append(1)\n",
    "                    labels.append(1)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9190cc92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d24563a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf9d82cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3333333333333333\n",
      "Precision: 0.3333333333333333\n",
      "Recall: 1.0\n",
      "AUC: 0.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# calculate accuracy, precision, recall, and AUC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "accuracy = accuracy_score(labels, preds)\n",
    "precision = precision_score(labels, preds)\n",
    "recall = recall_score(labels, preds)\n",
    "auc = roc_auc_score(labels, preds)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"AUC:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285e08a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
