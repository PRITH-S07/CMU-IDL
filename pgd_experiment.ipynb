{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "03ee7dc1",
      "metadata": {
        "id": "03ee7dc1"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "071db727",
      "metadata": {
        "id": "071db727"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = \"/content/drive/MyDrive/IDL Image Generation/data\"\n",
        "#OUTPUT_PATH = \"/content/drive/MyDrive/IDL Image Generation/images/classic_pgd_outputs\"\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/IDL Image Generation/images/attn_pgd_outputs\"\n",
        "MODEL_PATH = \"./CLIP_discriminator.pt\"\n",
        "GENERATORS = [\n",
        "    \"dalle\",\n",
        "    \"openjourney\",\n",
        "    \"stable_diff\",\n",
        "    \"openjourney_v4\",\n",
        "    \"titan\"\n",
        "]\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "EXPERIMENT_MODE = \"attn\" # \"pgd\" or \"patch\" or \"attn\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWel-PnCuT-s",
        "outputId": "98d388e5-718f-4004-c596-668b0e1d1b5f"
      },
      "id": "UWel-PnCuT-s",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6f728bcf",
      "metadata": {
        "id": "6f728bcf"
      },
      "outputs": [],
      "source": [
        "!pip install torchattacks --quiet\n",
        "!pip install transformers --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/MyDrive/IDL Image Generation\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeG8AV4VtMfy",
        "outputId": "b4198692-21a8-4482-b823-090284b52a53"
      },
      "id": "CeG8AV4VtMfy",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1SUnyLWY7LvpxPNxyFvip9Ae4S39ePRqJ/IDL Image Generation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "282f0f86",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "282f0f86",
        "outputId": "3809a1c6-11ae-46f4-992f-8b07f03335a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "import os\n",
        "import ast\n",
        "import csv\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "from AttentionPatchExtractor import AttentionPatchExtractor\n",
        "from CLIP_patch_pgd import CLIPPatchPGDAttack\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    average_precision_score,\n",
        ")\n",
        "\n",
        "from torchattacks import PGD\n",
        "from torchattacks.attack import Attack\n",
        "\n",
        "import torchvision\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd57ad31",
      "metadata": {
        "id": "fd57ad31"
      },
      "source": [
        "## Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "831d9188",
      "metadata": {
        "id": "831d9188"
      },
      "outputs": [],
      "source": [
        "class CLIPSVMDiscriminator(torch.nn.Module):\n",
        "    \"\"\"def __init__(self, model_name=\"openai/clip-vit-base-patch32\", device=None):\n",
        "        self.device = (\n",
        "            device\n",
        "            if device is not None\n",
        "            else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        )\n",
        "        print(\"Running on:\", self.device)\n",
        "        self.model = CLIPModel.from_pretrained(model_name)\n",
        "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        self.svm = SVC(kernel=\"linear\", C=1.0, probability=True)\n",
        "        self.svm_trained = False\"\"\"\n",
        "    def __init__(self, model_name=\"openai/clip-vit-base-patch32\", device=None):\n",
        "        super().__init__()\n",
        "        self.device = (\n",
        "            device\n",
        "            if device is not None\n",
        "            else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        )\n",
        "        print(\"Running on:\", self.device)\n",
        "        self.model = CLIPModel.from_pretrained(model_name)\n",
        "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        self.svm = SVC(kernel=\"linear\", C=1.0, probability=True)\n",
        "        self.svm_trained = False\n",
        "    def run_clip(self, imgs):\n",
        "        # inputs = self.processor(images=imgs, return_tensors=\"pt\").to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.vision_model(imgs)\n",
        "            image_features = outputs.last_hidden_state[:, 0, :]\n",
        "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "            return image_features.squeeze().cpu().numpy()\n",
        "\n",
        "    def train_svm(self, X_train, y_train):\n",
        "        self.svm.fit(X_train, y_train)\n",
        "        self.svm_trained = True\n",
        "        train_accuracy = self.svm.score(X_train, y_train)\n",
        "        print(f\"Training accuracy for discriminator: {train_accuracy:.4f}\")\n",
        "        return self.svm\n",
        "\n",
        "    def predict_from_embeddings(self, embeddings):\n",
        "        preds = self.svm.predict(embeddings)\n",
        "        probs = self.svm.predict_proba(embeddings)[:, 1]\n",
        "        return preds, probs\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        model = self.svm\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
        "        recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
        "        f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "        auc = roc_auc_score(y_test, y_pred_proba)\n",
        "        ap_per_class = []\n",
        "        for class_label in np.unique(y_test):\n",
        "            y_test_binary = (y_test == class_label).astype(int)\n",
        "            ap = average_precision_score(y_test_binary, y_pred_proba)\n",
        "            ap_per_class.append(ap)\n",
        "        map_score = np.mean(ap_per_class)\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1 Score: {f1:.4f}\")\n",
        "        print(f\"AUC: {auc:.4f}\")\n",
        "        print(f\"mAP: {map_score:.4f}\")\n",
        "        return {\n",
        "            \"accuracy\": accuracy,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1,\n",
        "            \"auc\": auc,\n",
        "            \"map\": map_score,\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "2eeb6c3a",
      "metadata": {
        "id": "2eeb6c3a"
      },
      "outputs": [],
      "source": [
        "model: CLIPSVMDiscriminator = torch.load(\n",
        "    MODEL_PATH, weights_only=False, map_location=DEVICE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.model"
      ],
      "metadata": {
        "id": "RT41wLtiz9UW"
      },
      "id": "RT41wLtiz9UW",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a132706d",
      "metadata": {
        "id": "a132706d"
      },
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "4c393549",
      "metadata": {
        "id": "4c393549"
      },
      "outputs": [],
      "source": [
        "class ArtEmbeddingDataset(Dataset):\n",
        "    def __init__(self, ai_only=False):\n",
        "        self.transform = torchvision.transforms.Resize((224, 224))\n",
        "        self.image_info = {}\n",
        "        for directory in GENERATORS:\n",
        "            if not os.path.exists(os.path.join(DATA_PATH, directory)):\n",
        "                print(f\"{directory} does not exist. Skipping.\")\n",
        "                continue\n",
        "            print(f\"{directory} has {len(os.listdir(os.path.join(DATA_PATH, directory)))} images.\")\n",
        "            for filepath in tqdm(os.listdir(os.path.join(DATA_PATH, directory)), desc=\"Loading \"+ directory):\n",
        "                full_path = os.path.join(DATA_PATH, directory, filepath)\n",
        "                if full_path.endswith(\".png\") or full_path.endswith(\".jpg\"):\n",
        "                    id_idx = filepath.rfind('_') + 1\n",
        "                    id = filepath[id_idx:-4]\n",
        "                    label = 1 if directory == \"real\" else 0\n",
        "                    self.image_info[full_path] = {\n",
        "                        \"generator\": directory,\n",
        "                        \"label\": label, # 0 = fake, 1 = real\n",
        "                        # \"id\": id,\n",
        "                        # \"data\": torchvision.io.read_image(full_path),\n",
        "                    }\n",
        "        self.paths = list(self.image_info.keys())\n",
        "\n",
        "    def preprocess_image(self, image_path):\n",
        "        # Load the image to torch\n",
        "        image = torchvision.io.read_image(image_path)\n",
        "        # image = self.transform(image)\n",
        "        # image = image.float() / 255.0  # Normalize to [0, 1]\n",
        "        return image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_info)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filepath = self.paths[idx]\n",
        "        label = self.image_info[filepath][\"label\"]\n",
        "        image = torchvision.io.read_image(filepath) #.convert(\"RGB\")\n",
        "        # image = self.transform(image)\n",
        "        # image = image.float() / 255.0\n",
        "\n",
        "        sample = {\"filepath\": filepath, \"data\": image, \"label\": label}\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "f006c54b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f006c54b",
        "outputId": "824bf188-7506-43ff-a8eb-74e69b47833e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dalle has 435 images.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading dalle: 100%|██████████| 435/435 [00:00<00:00, 320597.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjourney has 3376 images.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading openjourney: 100%|██████████| 3376/3376 [00:00<00:00, 371321.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stable_diff has 3345 images.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading stable_diff: 100%|██████████| 3345/3345 [00:00<00:00, 415554.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjourney_v4 has 3465 images.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading openjourney_v4: 100%|██████████| 3465/3465 [00:00<00:00, 434477.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "titan has 2058 images.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading titan: 100%|██████████| 2058/2058 [00:00<00:00, 407048.84it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12679"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "dataset = ArtEmbeddingDataset(ai_only=True)\n",
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "e8dc3b6a",
      "metadata": {
        "id": "e8dc3b6a"
      },
      "outputs": [],
      "source": [
        "TRANSFORM = torchvision.transforms.Resize((224, 224))\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images = [item[\"data\"] for item in batch]\n",
        "    labels = [item[\"label\"] for item in batch]\n",
        "    paths = [item[\"filepath\"] for item in batch]\n",
        "    labels = torch.tensor(labels)\n",
        "\n",
        "    images = torch.stack(images)\n",
        "    images = images.float() / 255.0  # Normalize to [0, 1]\n",
        "    images = TRANSFORM(images)\n",
        "\n",
        "    images = images.to(DEVICE)\n",
        "    labels = labels.to(DEVICE)\n",
        "\n",
        "    return images, labels, paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "1b62963b",
      "metadata": {
        "id": "1b62963b"
      },
      "outputs": [],
      "source": [
        "dataloader = DataLoader(\n",
        "    dataset=dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22f70835",
      "metadata": {
        "id": "22f70835"
      },
      "source": [
        "## Attacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "17d44fd7",
      "metadata": {
        "id": "17d44fd7"
      },
      "outputs": [],
      "source": [
        "import torchvision.utils as vutils\n",
        "\n",
        "\n",
        "class CLIPPGDAttack(PGD):\n",
        "    def __init__(self, model, svm, eps=8 / 255, alpha=2 / 255, steps=10, random_start=True ):\n",
        "        super().__init__(model, eps, alpha, steps, random_start)\n",
        "        device = self.device\n",
        "        self.svm_weights = torch.FloatTensor(svm.coef_[0]).to(device)\n",
        "        self.svm_bias = torch.tensor(svm.intercept_[0],dtype=torch.float32, device=device)\n",
        "        # self.register_buffer('svm_weights', self.svm_weights)\n",
        "        # self.register_buffer('svm_bias', self.svm_bias)\n",
        "\n",
        "\n",
        "    def get_logits(self, inputs):\n",
        "        if self._normalization_applied is False:\n",
        "            inputs = self.normalize(inputs)\n",
        "\n",
        "        # Get image features from the vision model\n",
        "        vision_outputs = self.model.vision_model(inputs)\n",
        "        image_features = vision_outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        return image_features\n",
        "\n",
        "    def svm_boundary_loss(self, clip_embedding):\n",
        "        # Distance to decision boundary (negative = wrong side)\n",
        "        if self.svm_weights is None or self.svm_bias is None:\n",
        "            raise ValueError(\"SVM weights and bias not set. Call set_svm_params() first.\")\n",
        "\n",
        "        distance = torch.matmul(clip_embedding, self.svm_weights) + self.svm_bias\n",
        "        # Loss is higher when distance is positive (correct classification)\n",
        "        return -distance  # Maximize to cross boundary\n",
        "\n",
        "    def forward(self, images, labels):\n",
        "        \"\"\"\n",
        "        Override forward method to use custom loss function\n",
        "        \"\"\"\n",
        "        images = images.clone().detach().to(self.device)\n",
        "\n",
        "        adv_images = images.clone().detach()\n",
        "\n",
        "        if self.random_start:\n",
        "            # Starting at a uniformly random point\n",
        "            adv_images = adv_images + torch.empty_like(adv_images).uniform_(-self.eps, self.eps)\n",
        "            adv_images = torch.clamp(adv_images, min=0, max=1).detach()\n",
        "\n",
        "        for _ in range(self.steps):\n",
        "            adv_images.requires_grad = True\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = self.get_logits(adv_images)\n",
        "\n",
        "            # Calculate loss using our custom SVM boundary loss\n",
        "            loss = self.svm_boundary_loss(outputs).mean()\n",
        "            # print(f\"Loss: {loss.item():.6f}\")\n",
        "\n",
        "            # Backward pass\n",
        "            grad = torch.autograd.grad(loss, adv_images,\n",
        "                                      retain_graph=False, create_graph=False)[0]\n",
        "\n",
        "            adv_images = adv_images.detach() - self.alpha * grad.sign()\n",
        "            delta = torch.clamp(adv_images - images, min=-self.eps, max=self.eps)\n",
        "            adv_images = torch.clamp(images + delta, min=0, max=1).detach()\n",
        "\n",
        "        return adv_images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "26cb2dde",
      "metadata": {
        "id": "26cb2dde"
      },
      "outputs": [],
      "source": [
        "\"\"\"if EXPERIMENT_MODE == \"pgd\":\n",
        "    attack = CLIPPGDAttack(model.model, model.svm, eps=8 / 255, alpha=2 / 255, steps=10, random_start=True)\n",
        "elif EXPERIMENT_MODE == \"patch\":\n",
        "    attack = CLIPPatchPGDAttack(model.model, model.svm, eps=8 / 255, alpha=2 / 255, steps=10, patch_selection_strategy= 'grad', random_start=True)\n",
        "elif EXPERIMENT_MODE == \"attn\":\n",
        "    attack = CLIPPatchPGDAttack(model.model, model.svm, eps=8 / 255, alpha=2 / 255, steps=10, patch_selection_strategy= 'attention', random_start=True)\n",
        "else:\n",
        "    raise ValueError(f\"Invalid experiment mode:\", EXPERIMENT_MODE)\"\"\"\n",
        "if EXPERIMENT_MODE == \"pgd\":\n",
        "    attack = CLIPPGDAttack(model.model, model.svm, eps=8 / 255, alpha=2 / 255, steps=20, patch_size=32, random_start=True)\n",
        "elif EXPERIMENT_MODE == \"patch\":\n",
        "    attack = CLIPPatchPGDAttack(model.model, model.svm, eps=.3, alpha=2 / 255, steps=20, patch_size=32, num_patches = 5, patch_selection_strategy= 'grad', random_start=True)\n",
        "elif EXPERIMENT_MODE == \"attn\":\n",
        "    attack = CLIPPatchPGDAttack(model.model, model.svm, eps=.3, alpha=2 / 255, steps=20, patch_size=32, num_patches = 5, patch_selection_strategy= 'attention', random_start=True)\n",
        "else:\n",
        "    raise ValueError(f\"Invalid experiment mode:\", EXPERIMENT_MODE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "305348c7",
      "metadata": {
        "id": "305348c7"
      },
      "source": [
        "## Generate Adversarial Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "a9f4ecb2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9f4ecb2",
        "outputId": "0ac86262-8650-47ae-d9a1-0ac7ad9553b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/6340 [00:00<?, ?it/s]CLIPModel is using CLIPSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "100%|██████████| 6340/6340 [1:14:23<00:00,  1.42it/s]\n"
          ]
        }
      ],
      "source": [
        "for i, (images, labels, paths) in enumerate(tqdm(dataloader)):\n",
        "    images = images.to(DEVICE)\n",
        "    labels = labels.to(DEVICE)\n",
        "\n",
        "    # Generate adversarial examples\n",
        "    adv_images = attack(images, labels)\n",
        "    # print(len(adv_images))\n",
        "    # print(len(images))\n",
        "\n",
        "    # Save the adversarial images\n",
        "    for j in range(len(images)):\n",
        "        image_path = paths[j]\n",
        "        basename = os.path.basename(image_path)\n",
        "        adv_image_path = os.path.join(OUTPUT_PATH, f\"adv_{EXPERIMENT_MODE}_{basename}\")\n",
        "        torchvision.utils.save_image(adv_images[j], adv_image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45f40a79",
      "metadata": {
        "id": "45f40a79"
      },
      "source": [
        "## Test Attack Effectiveness"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###FOR ATTN PGD\n",
        "# Replace the Test Attack Effectiveness section with this batch processing approach\n",
        "\n",
        "# Define batch size for testing\n",
        "TEST_BATCH_SIZE = 16  # Adjust based on your GPU memory\n",
        "\n",
        "# Create lists to store all embeddings and labels\n",
        "all_clip_embeddings = []\n",
        "all_labels = []\n",
        "\n",
        "# Process adversarial images in batches\n",
        "adv_images = []\n",
        "adv_labels = []\n",
        "for file in tqdm(os.listdir(OUTPUT_PATH), desc=\"Loading generated images\"):\n",
        "    adv_images.append(torchvision.io.read_image(os.path.join(OUTPUT_PATH, file)))\n",
        "    adv_labels.append(0)\n",
        "\n",
        "# Process in batches\n",
        "for i in range(0, len(adv_images), TEST_BATCH_SIZE):\n",
        "    batch_images = adv_images[i:i+TEST_BATCH_SIZE]\n",
        "    batch_labels = adv_labels[i:i+TEST_BATCH_SIZE]\n",
        "\n",
        "    # Stack and preprocess\n",
        "    batch_images = torch.stack(batch_images).to(DEVICE)\n",
        "    batch_images = TRANSFORM(batch_images)\n",
        "    batch_images = batch_images.float() / 255.0\n",
        "\n",
        "    # Get embeddings\n",
        "    with torch.no_grad():\n",
        "        batch_embeddings = model.run_clip(batch_images)\n",
        "\n",
        "    # Store results (move to CPU to free GPU memory)\n",
        "    all_clip_embeddings.append(batch_embeddings)\n",
        "    all_labels.extend(batch_labels)\n",
        "\n",
        "    # Clear cache\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Move processed images off memory\n",
        "    del batch_images\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Process real images in batches\n",
        "real_images = []\n",
        "real_labels = []\n",
        "for file in tqdm(os.listdir(os.path.join(DATA_PATH, \"real\")), desc=\"Loading real images\"):\n",
        "    real_images.append(torchvision.io.read_image(os.path.join(DATA_PATH, \"real\", file)))\n",
        "    real_labels.append(1)\n",
        "\n",
        "# Process in batches\n",
        "for i in range(0, len(real_images), TEST_BATCH_SIZE):\n",
        "    batch_images = real_images[i:i+TEST_BATCH_SIZE]\n",
        "    batch_labels = real_labels[i:i+TEST_BATCH_SIZE]\n",
        "\n",
        "    # Stack and preprocess\n",
        "    batch_images = torch.stack(batch_images).to(DEVICE)\n",
        "    batch_images = TRANSFORM(batch_images)\n",
        "    batch_images = batch_images.float() / 255.0\n",
        "\n",
        "    # Get embeddings\n",
        "    with torch.no_grad():\n",
        "        batch_embeddings = model.run_clip(batch_images)\n",
        "\n",
        "    # Store results (move to CPU to free GPU memory)\n",
        "    all_clip_embeddings.append(batch_embeddings)\n",
        "    all_labels.extend(batch_labels)\n",
        "\n",
        "    # Clear cache\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Move processed images off memory\n",
        "    del batch_images\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Combine all embeddings and evaluate\n",
        "clip_embeddings = np.vstack(all_clip_embeddings)\n",
        "print(clip_embeddings.shape)\n",
        "\n",
        "# Evaluate\n",
        "model.evaluate(clip_embeddings, all_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raBWpRcB0Gf8",
        "outputId": "524aacde-1593-4c0f-e791-efd6ed32cc0f"
      },
      "id": "raBWpRcB0Gf8",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading generated images: 100%|██████████| 12679/12679 [02:37<00:00, 80.68it/s] \n",
            "Loading real images: 100%|██████████| 3633/3633 [00:38<00:00, 94.69it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16312, 768)\n",
            "Accuracy: 0.4342\n",
            "Precision: 0.7338\n",
            "Recall: 0.4342\n",
            "F1 Score: 0.4518\n",
            "AUC: 0.6351\n",
            "mAP: 0.5116\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.4341589014222658,\n",
              " 'precision': 0.7338212883171638,\n",
              " 'recall': 0.4341589014222658,\n",
              " 'f1': 0.45181728329871035,\n",
              " 'auc': np.float64(0.6351374548233675),\n",
              " 'map': np.float64(0.511593211857198)}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###FOR CLASSIC PGD\n",
        "# Replace the Test Attack Effectiveness section with this batch processing approach\n",
        "\n",
        "# Define batch size for testing\n",
        "TEST_BATCH_SIZE = 16  # Adjust based on your GPU memory\n",
        "\n",
        "# Create lists to store all embeddings and labels\n",
        "all_clip_embeddings = []\n",
        "all_labels = []\n",
        "\n",
        "# Process adversarial images in batches\n",
        "adv_images = []\n",
        "adv_labels = []\n",
        "for file in tqdm(os.listdir(OUTPUT_PATH), desc=\"Loading generated images\"):\n",
        "    adv_images.append(torchvision.io.read_image(os.path.join(OUTPUT_PATH, file)))\n",
        "    adv_labels.append(0)\n",
        "\n",
        "# Process in batches\n",
        "for i in range(0, len(adv_images), TEST_BATCH_SIZE):\n",
        "    batch_images = adv_images[i:i+TEST_BATCH_SIZE]\n",
        "    batch_labels = adv_labels[i:i+TEST_BATCH_SIZE]\n",
        "\n",
        "    # Stack and preprocess\n",
        "    batch_images = torch.stack(batch_images).to(DEVICE)\n",
        "    batch_images = TRANSFORM(batch_images)\n",
        "    batch_images = batch_images.float() / 255.0\n",
        "\n",
        "    # Get embeddings\n",
        "    with torch.no_grad():\n",
        "        batch_embeddings = model.run_clip(batch_images)\n",
        "\n",
        "    # Store results (move to CPU to free GPU memory)\n",
        "    all_clip_embeddings.append(batch_embeddings)\n",
        "    all_labels.extend(batch_labels)\n",
        "\n",
        "    # Clear cache\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Move processed images off memory\n",
        "    del batch_images\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Process real images in batches\n",
        "real_images = []\n",
        "real_labels = []\n",
        "for file in tqdm(os.listdir(os.path.join(DATA_PATH, \"real\")), desc=\"Loading real images\"):\n",
        "    real_images.append(torchvision.io.read_image(os.path.join(DATA_PATH, \"real\", file)))\n",
        "    real_labels.append(1)\n",
        "\n",
        "# Process in batches\n",
        "for i in range(0, len(real_images), TEST_BATCH_SIZE):\n",
        "    batch_images = real_images[i:i+TEST_BATCH_SIZE]\n",
        "    batch_labels = real_labels[i:i+TEST_BATCH_SIZE]\n",
        "\n",
        "    # Stack and preprocess\n",
        "    batch_images = torch.stack(batch_images).to(DEVICE)\n",
        "    batch_images = TRANSFORM(batch_images)\n",
        "    batch_images = batch_images.float() / 255.0\n",
        "\n",
        "    # Get embeddings\n",
        "    with torch.no_grad():\n",
        "        batch_embeddings = model.run_clip(batch_images)\n",
        "\n",
        "    # Store results (move to CPU to free GPU memory)\n",
        "    all_clip_embeddings.append(batch_embeddings)\n",
        "    all_labels.extend(batch_labels)\n",
        "\n",
        "    # Clear cache\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Move processed images off memory\n",
        "    del batch_images\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Combine all embeddings and evaluate\n",
        "clip_embeddings = np.vstack(all_clip_embeddings)\n",
        "print(clip_embeddings.shape)\n",
        "\n",
        "# Evaluate\n",
        "model.evaluate(clip_embeddings, all_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XJi1TtWbW4k",
        "outputId": "7d04cac7-a81c-4902-cb0b-44ff2806c3eb"
      },
      "id": "0XJi1TtWbW4k",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading generated images: 100%|██████████| 12679/12679 [02:06<00:00, 100.44it/s]\n",
            "Loading real images: 100%|██████████| 3633/3633 [00:36<00:00, 100.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16312, 768)\n",
            "Accuracy: 0.1854\n",
            "Precision: 0.0429\n",
            "Recall: 0.1854\n",
            "F1 Score: 0.0697\n",
            "AUC: 0.0110\n",
            "mAP: 0.5588\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.18544629720451203,\n",
              " 'precision': 0.04290157907182834,\n",
              " 'recall': 0.18544629720451203,\n",
              " 'f1': 0.0696826185803374,\n",
              " 'auc': np.float64(0.011045418921169952),\n",
              " 'map': np.float64(0.5587876268380603)}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "53b2a114",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "53b2a114",
        "outputId": "f4673e38-cb6f-4d74-97f5-9edf5b51916f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'images = []\\nlabels = []\\nfor file in tqdm(os.listdir(OUTPUT_PATH), desc=\"Loading generated images\"):\\n    images.append(torchvision.io.read_image(os.path.join(OUTPUT_PATH, file)))\\n    labels.append(0)\\nfor file in tqdm(os.listdir(os.path.join(DATA_PATH, \"real\")), desc=\"Loading real images\"):\\n    images.append(TRANSFORM(torchvision.io.read_image(os.path.join(DATA_PATH, \"real\", file))))\\n    labels.append(1)\\n\\n\\nimages = torch.stack(images).to(DEVICE)\\nimages = images.float() / 255.0  # Normalize to [0, 1]\\n\\nclip_embeddings = model.run_clip(images)\\nprint(clip_embeddings.shape)\\n\\nmodel.evaluate(clip_embeddings, labels)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "'''images = []\n",
        "labels = []\n",
        "for file in tqdm(os.listdir(OUTPUT_PATH), desc=\"Loading generated images\"):\n",
        "    images.append(torchvision.io.read_image(os.path.join(OUTPUT_PATH, file)))\n",
        "    labels.append(0)\n",
        "for file in tqdm(os.listdir(os.path.join(DATA_PATH, \"real\")), desc=\"Loading real images\"):\n",
        "    images.append(TRANSFORM(torchvision.io.read_image(os.path.join(DATA_PATH, \"real\", file))))\n",
        "    labels.append(1)\n",
        "\n",
        "\n",
        "images = torch.stack(images).to(DEVICE)\n",
        "images = images.float() / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "clip_embeddings = model.run_clip(images)\n",
        "print(clip_embeddings.shape)\n",
        "\n",
        "model.evaluate(clip_embeddings, labels)'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images = []\n",
        "labels = []\n",
        "adv_files = os.listdir(OUTPUT_PATH)\n",
        "for file in tqdm(adv_files, desc=\"Loading generated images\"):\n",
        "    images.append(torchvision.io.read_image(os.path.join(OUTPUT_PATH, file)))\n",
        "    labels.append(0)\n",
        "for file in tqdm(os.listdir(os.path.join(DATA_PATH, \"real\")), desc=\"Loading real images\"):\n",
        "    images.append(TRANSFORM(torchvision.io.read_image(os.path.join(DATA_PATH, \"real\", file))))\n",
        "    labels.append(1)\n",
        "\n",
        "images = torch.stack(images).to(DEVICE)\n",
        "images = images.float() / 255.0  # Normalize to [0, 1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Et_-_pRyg7Lt",
        "outputId": "0afdf880-9783-4e7b-8b75-64a232162ad5"
      },
      "id": "Et_-_pRyg7Lt",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading generated images: 100%|██████████| 12679/12679 [02:02<00:00, 103.52it/s]\n",
            "Loading real images: 100%|██████████| 3633/3633 [00:41<00:00, 86.74it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "85c511a5",
      "metadata": {
        "id": "85c511a5"
      },
      "outputs": [],
      "source": [
        "attn_file_paths = os.listdir(OUTPUT_PATH) + os.listdir(os.path.join(DATA_PATH, \"real\"))\n",
        "ids = [(f.split('.jpg')[0]).split('_')[-1] for f in attn_file_paths]\n",
        "\n",
        "generator = []\n",
        "for f in attn_file_paths:\n",
        "  try:\n",
        "    x = (f.split('.jpg')[0]).split('_')[2]\n",
        "    generator.append(x)\n",
        "  except:\n",
        "    generator.append('real')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clip_batch_size = 2048\n",
        "all_embeddings = []\n",
        "\n",
        "for i in range(0, len(images), clip_batch_size):\n",
        "  if i + clip_batch_size > len(images):\n",
        "    images_batch = images[i:]\n",
        "  else:\n",
        "    images_batch = images[i:i+clip_batch_size]\n",
        "\n",
        "  clip_embeddings = model.run_clip(images_batch)\n",
        "  all_embeddings.extend(clip_embeddings)\n",
        "\n",
        "preds, _ = model.predict_from_embeddings(all_embeddings)"
      ],
      "metadata": {
        "id": "zyeev6p8gA37"
      },
      "id": "zyeev6p8gA37",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'id': ids,\n",
        "    'generator': generator,\n",
        "    'embedding': all_embeddings,\n",
        "    'label': labels,\n",
        "    'prediction': preds,\n",
        "    'attack_type': ['grad'] * len(preds)\n",
        "})\n",
        "\n",
        "df.to_csv('./attn_pgd_clip_embeddings.csv', index=False)"
      ],
      "metadata": {
        "id": "9ERhHFhqg0hw"
      },
      "id": "9ERhHFhqg0hw",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yhfatp21hw1U"
      },
      "id": "Yhfatp21hw1U",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}