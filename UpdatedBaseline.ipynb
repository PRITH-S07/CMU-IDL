{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline for CLIP AI Detection Adversarial Attack"
      ],
      "metadata": {
        "id": "zJENsdUGGHcM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "cwbcNfoEGUT6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7SwWf4G4ebZ",
        "outputId": "2fadf06c-9152-4180-cb13-cb4dd710cee6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "AziwBTP48KWo"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch --quiet\n",
        "!pip install torchattacks --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zMkDJ-xI4iJj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import ast\n",
        "import csv\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
        "\n",
        "from torchattacks import PGD\n",
        "import torchvision\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FJk4lpvt3PX6"
      },
      "outputs": [],
      "source": [
        "DIFFUSION_MODELS = [\"openjourney\", \"titan\", \"dalle\", \"real\", \"openjourney_v4\", \"stable_diff\"]\n",
        "ROOT = '/content/drive/MyDrive/IDL Image Generation'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process Images with CLIP"
      ],
      "metadata": {
        "id": "89x0uZAPGb_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9V2rCeXs3pEm",
        "outputId": "d2b282f7-4e80-4510-bb62-65312afef32d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access 'data': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gV80yynV3KUq"
      },
      "outputs": [],
      "source": [
        "def collect_images(generators):\n",
        "  generated_images = {}\n",
        "  for directory in generators:\n",
        "    print(directory, len(os.listdir(os.path.join(ROOT, \"data\", directory))))\n",
        "    for filepath in os.listdir(os.path.join(ROOT, \"data\", directory)):\n",
        "        full_path = os.path.join(ROOT, \"data\", directory, filepath)\n",
        "        id_idx = filepath.rfind('_') + 1\n",
        "        id = filepath[id_idx:-4]\n",
        "        label = 1 if directory == \"real\" else 0\n",
        "        generated_images[full_path] = {\n",
        "            \"generator\": directory,\n",
        "            \"label\": label, # 0 = fake, 1 = real\n",
        "            \"id\": id,\n",
        "        }\n",
        "  return generated_images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "VeJb_dwf4ZBY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ca89f27-99a9-416f-f951-731ec294b0b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjourney 3376\n",
            "titan 2058\n",
            "dalle 435\n",
            "real 3633\n",
            "openjourney_v4 3465\n",
            "stable_diff 3345\n"
          ]
        }
      ],
      "source": [
        "images = collect_images(DIFFUSION_MODELS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fkDSUmX6yQC",
        "outputId": "d58725b5-38e3-4af4-fa75-4c1488a02d4f",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CLIPModel(\n",
              "  (text_model): CLIPTextTransformer(\n",
              "    (embeddings): CLIPTextEmbeddings(\n",
              "      (token_embedding): Embedding(49408, 512)\n",
              "      (position_embedding): Embedding(77, 512)\n",
              "    )\n",
              "    (encoder): CLIPEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x CLIPEncoderLayer(\n",
              "          (self_attn): CLIPSdpaAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): QuickGELUActivation()\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (vision_model): CLIPVisionTransformer(\n",
              "    (embeddings): CLIPVisionEmbeddings(\n",
              "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
              "      (position_embedding): Embedding(50, 768)\n",
              "    )\n",
              "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (encoder): CLIPEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x CLIPEncoderLayer(\n",
              "          (self_attn): CLIPSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): QuickGELUActivation()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
              "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "print(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7lTfd42V60Iv"
      },
      "outputs": [],
      "source": [
        "def run_clip(image_path):\n",
        "  with Image.open(image_path) as img:\n",
        "    img = img.convert(\"RGB\")\n",
        "    if img.size != (512, 512):\n",
        "      print(f\"Resizing {image_path} to 512x512\")\n",
        "      img = img.resize((512, 512))\n",
        "      img.save(image_path)\n",
        "  # inputs = processor(images=img, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "  inputs = processor(images=img, return_tensors=\"pt\")\n",
        "  inputs = inputs.to(device)\n",
        "  with torch.no_grad():\n",
        "    # image_features = model.get_image_features(**inputs)\n",
        "    outputs = model.vision_model(inputs.pixel_values)\n",
        "    image_features = outputs.last_hidden_state[:, 0, :]\n",
        "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "    return image_features.squeeze().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CyXYhHNvdniX"
      },
      "outputs": [],
      "source": [
        "def get_processed_filepaths(csv_path: str):\n",
        "    processed_filepaths = set()\n",
        "    if not os.path.isfile(csv_path):\n",
        "        return set()\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        df = df[df[\"Generator\"].isin(DIFFUSION_MODELS)]\n",
        "        df = df.drop_duplicates(subset=[\"Generator\", \"Id\"])\n",
        "        processed_filepaths = set(df[\"Filepath\"].values)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    return processed_filepaths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mtO0L5VdWaG0"
      },
      "outputs": [],
      "source": [
        "def process_images_and_log(image_dict: dict, csv_fname: str) -> None:\n",
        "    processed_filepaths = get_processed_filepaths(os.path.join(ROOT, csv_fname))\n",
        "    print(f\"{len(processed_filepaths)} files have already been processed\")\n",
        "    for path, info in tqdm(image_dict.items(), desc=\"Embedding Images\"):\n",
        "      if path in processed_filepaths:\n",
        "        # print(f\"{info['generator']}_{info['id']} has already been processed\")\n",
        "        continue\n",
        "\n",
        "      encoding = run_clip(path)\n",
        "\n",
        "      data = [path, info[\"id\"], info[\"generator\"], str(encoding), info[\"label\"]]\n",
        "\n",
        "      csv_path = os.path.join(ROOT, csv_fname)\n",
        "      file_exists = os.path.isfile(csv_path)\n",
        "      with open(csv_path, 'a', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        if not file_exists:\n",
        "            writer.writerow([\"Filepath\", \"Id\", \"Generator\", \"Features\", \"Label\"])\n",
        "        writer.writerow(data)\n",
        "      # print(f\"{info['generator']}_{info['id']} encoded and logged to csv\")\n",
        "    print(f\"All input images encoded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "i2P7Gy2fZ_Hn",
        "outputId": "e9d2d622-9006-49d2-aa7e-a89e90b5a269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16313 files have already been processed\n"
          ]
        }
      ],
      "source": [
        "# process_images_and_log(images, \"CLIP_embeddings.csv\")\n",
        "processed_filepaths = get_processed_filepaths(os.path.join(ROOT, \"CLIP_embeddings.csv\"))\n",
        "print(f\"{len(processed_filepaths)} files have already been processed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train SVM for AI Image Detection"
      ],
      "metadata": {
        "id": "veHjUmu6GfZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPSVMDiscriminator:\n",
        "    def __init__(self, model_name=\"openai/clip-vit-base-patch32\", device=None):\n",
        "        self.device = device if device is not None else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(\"Running on:\", self.device)\n",
        "        self.model = CLIPModel.from_pretrained(model_name)\n",
        "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        self.svm = SVC(kernel=\"linear\", C=1.0, probability=True)\n",
        "        self.svm_trained = False\n",
        "\n",
        "    def run_clip(self, imgs):\n",
        "        inputs = self.processor(images=imgs, return_tensors=\"pt\").to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.vision_model(inputs.pixel_values)\n",
        "            image_features = outputs.last_hidden_state[:, 0, :]\n",
        "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "            return image_features.squeeze().cpu().numpy()\n",
        "\n",
        "    def train_svm(self, X_train, y_train):\n",
        "        self.svm.fit(X_train, y_train)\n",
        "        self.svm_trained = True\n",
        "        train_accuracy = self.svm.score(X_train, y_train)\n",
        "        print(f\"Training accuracy for discriminator: {train_accuracy:.4f}\")\n",
        "        return self.svm\n",
        "\n",
        "    def predict_from_embeddings(self, embeddings):\n",
        "        preds = self.svm.predict(embeddings)\n",
        "        probs = self.svm.predict_proba(embeddings)[:, 1]\n",
        "        return preds, probs\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        model = self.svm\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='weighted')\n",
        "        recall = recall_score(y_test, y_pred, average='weighted')\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "        auc = roc_auc_score(y_test, y_pred_proba)\n",
        "        ap_per_class = []\n",
        "        for class_label in np.unique(y_test):\n",
        "            y_test_binary = (y_test == class_label).astype(int)\n",
        "            ap = average_precision_score(y_test_binary, y_pred_proba)\n",
        "            ap_per_class.append(ap)\n",
        "        map_score = np.mean(ap_per_class)\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1 Score: {f1:.4f}\")\n",
        "        print(f\"AUC: {auc:.4f}\")\n",
        "        print(f\"mAP: {map_score:.4f}\")\n",
        "        return {\n",
        "            \"accuracy\": accuracy,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1,\n",
        "            \"auc\": auc,\n",
        "            \"map\": map_score\n",
        "        }"
      ],
      "metadata": {
        "id": "A58hsAQWHKMI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": true,
        "id": "dIqB_Cu49nH4",
        "outputId": "238643be-219d-4b92-e8c8-34d81a23446b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            Filepath     Id    Generator  \\\n",
              "0  /content/drive/MyDrive/IDL Image Generation/da...  93489  openjourney   \n",
              "1  /content/drive/MyDrive/IDL Image Generation/da...  93500  openjourney   \n",
              "2  /content/drive/MyDrive/IDL Image Generation/da...  93502  openjourney   \n",
              "3  /content/drive/MyDrive/IDL Image Generation/da...  93511  openjourney   \n",
              "4  /content/drive/MyDrive/IDL Image Generation/da...  93523  openjourney   \n",
              "\n",
              "                                            Features  Label  \n",
              "0  [-0.0115892114, 0.0602126196, -0.0306698009, -...      0  \n",
              "1  [-0.0148345735, 0.0203769933, -0.00583226699, ...      0  \n",
              "2  [-0.0165445693, 0.00986933149, -0.0408154577, ...      0  \n",
              "3  [-0.0157231297, 0.0540607572, -0.00418719556, ...      0  \n",
              "4  [0.0038340704, -0.000587593589, 0.0121148545, ...      0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-02a57d38-f7cb-4436-a8de-395f0c8177dc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Filepath</th>\n",
              "      <th>Id</th>\n",
              "      <th>Generator</th>\n",
              "      <th>Features</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/MyDrive/IDL Image Generation/da...</td>\n",
              "      <td>93489</td>\n",
              "      <td>openjourney</td>\n",
              "      <td>[-0.0115892114, 0.0602126196, -0.0306698009, -...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/MyDrive/IDL Image Generation/da...</td>\n",
              "      <td>93500</td>\n",
              "      <td>openjourney</td>\n",
              "      <td>[-0.0148345735, 0.0203769933, -0.00583226699, ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/drive/MyDrive/IDL Image Generation/da...</td>\n",
              "      <td>93502</td>\n",
              "      <td>openjourney</td>\n",
              "      <td>[-0.0165445693, 0.00986933149, -0.0408154577, ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/drive/MyDrive/IDL Image Generation/da...</td>\n",
              "      <td>93511</td>\n",
              "      <td>openjourney</td>\n",
              "      <td>[-0.0157231297, 0.0540607572, -0.00418719556, ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/drive/MyDrive/IDL Image Generation/da...</td>\n",
              "      <td>93523</td>\n",
              "      <td>openjourney</td>\n",
              "      <td>[0.0038340704, -0.000587593589, 0.0121148545, ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-02a57d38-f7cb-4436-a8de-395f0c8177dc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-02a57d38-f7cb-4436-a8de-395f0c8177dc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-02a57d38-f7cb-4436-a8de-395f0c8177dc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-be29c27d-1b65-4f74-8b6d-2c6db90c407f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-be29c27d-1b65-4f74-8b6d-2c6db90c407f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-be29c27d-1b65-4f74-8b6d-2c6db90c407f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 16313,\n  \"fields\": [\n    {\n      \"column\": \"Filepath\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 16313,\n        \"samples\": [\n          \"/content/drive/MyDrive/IDL Image Generation/data/openjourney/OpenJourney_9669.jpg\",\n          \"/content/drive/MyDrive/IDL Image Generation/data/titan/titan_71032.jpg\",\n          \"/content/drive/MyDrive/IDL Image Generation/data/real/67363.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7098,\n        \"samples\": [\n          \"93755.jpg\",\n          \"21504\",\n          \"51176.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Generator\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"openjourney\",\n          \"titan\",\n          \"stable_diff\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Features\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "def string_to_np(feature_str):\n",
        "    embedding_list = np.fromstring(feature_str[1:-1], sep=' ')\n",
        "    assert len(embedding_list) == 768\n",
        "    return np.array(embedding_list)\n",
        "\n",
        "\n",
        "df = pd.read_csv(os.path.join(ROOT, \"CLIP_embeddings.csv\"), usecols=range(5))\n",
        "# Filter df for rows where Generator is in DIFFUSION MODELS\n",
        "df = df[df[\"Generator\"].isin(DIFFUSION_MODELS)]\n",
        "df = df.drop_duplicates(subset=[\"Filepath\", \"Id\", \"Generator\", \"Label\"])\n",
        "df[\"Features\"] = df[\"Features\"].apply(string_to_np).to_numpy()\n",
        "X = np.stack(df[\"Features\"])\n",
        "y = df[\"Label\"].to_numpy()\n",
        "\n",
        "# df[\"Generator\"].value_counts()\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
      ],
      "metadata": {
        "id": "AX_xRmfqGSvd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsDsi5yHJhCg",
        "outputId": "2c6cfa20-eae5-4655-8ea2-a9e0608c2f59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16313, 768) (16313,)\n"
          ]
        }
      ],
      "source": [
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEJb3ouW92sT",
        "outputId": "b02bc0fc-8550-43fb-ecbc-ce67513e284c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on: cuda\n",
            "Training accuracy for discriminator: 0.9976\n",
            "Testing Metrics:\n",
            "Accuracy: 0.9979\n",
            "Precision: 0.9979\n",
            "Recall: 0.9979\n",
            "F1 Score: 0.9979\n",
            "AUC: 0.9998\n",
            "mAP: 0.7854\n"
          ]
        }
      ],
      "source": [
        "model = CLIPSVMDiscriminator()\n",
        "\n",
        "\n",
        "model.train_svm(X_train, y_train)\n",
        "print(\"Testing Metrics:\")\n",
        "metrics = model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, os.path.join(ROOT, \"CLIP_discriminator.pt\"))"
      ],
      "metadata": {
        "id": "cc2d1OfO-O_W"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate method on Unseen Generators"
      ],
      "metadata": {
        "id": "N6h1evhBGlfE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Rqm9mk25BnBA"
      },
      "outputs": [],
      "source": [
        "# for generator in [\"openjourney\", \"titan\", \"dalle\", \"openjourney_v4\"]:\n",
        "#   if generator == \"real\":\n",
        "#     continue\n",
        "\n",
        "#   test_df = df[df['Generator'] == generator]\n",
        "#   train_df = df[df['Generator'] != generator]\n",
        "\n",
        "#   # Reset the index for both partitions\n",
        "#   test_df = test_df.reset_index(drop=True)\n",
        "#   train_df = train_df.reset_index(drop=True)\n",
        "\n",
        "#   X_test = np.stack(test_df[\"Features\"].apply(string_to_np).to_numpy())\n",
        "#   y_test = test_df[\"Label\"].to_numpy()\n",
        "\n",
        "#   X_train = np.stack(train_df[\"Features\"].apply(string_to_np).to_numpy())\n",
        "#   y_train = train_df[\"Label\"].to_numpy()\n",
        "\n",
        "#   print(f\"Results for no-{generator} model\")\n",
        "#   svm = train(X_train, y_train)\n",
        "#   print(f\"Results on {generator}:\")\n",
        "#   metrics = evaluate(svm, X_test, y_test)\n",
        "#   print(\"******************************************\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloaders"
      ],
      "metadata": {
        "id": "N--5TYimyEQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ArtEmbeddingDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A dataset class for loading art image embeddings and labels from a CSV file.\n",
        "\n",
        "    Expected CSV columns:\n",
        "      - \"Filepath\": location of the original image.\n",
        "      - \"Features\": string representation of the image embedding vector.\n",
        "      - \"Label\": the label indicating whether the image is real or AI–generated.\n",
        "    \"\"\"\n",
        "    def __init__(self, csv_file, transform=None, ai_only=False):\n",
        "        df = pd.read_csv(os.path.join(ROOT, \"CLIP_embeddings.csv\"), usecols=range(5))\n",
        "        df = df[df[\"Generator\"].isin(DIFFUSION_MODELS)]\n",
        "        df = df.drop_duplicates(subset=[\"Filepath\", \"Id\", \"Generator\", \"Label\"])\n",
        "        df[\"Features\"] = df[\"Features\"].apply(string_to_np).to_numpy()\n",
        "\n",
        "        self.data = df\n",
        "        self.transform = transform\n",
        "\n",
        "        for g in DIFFUSION_MODELS:\n",
        "          os.makedirs(os.path.join(\"data\", \"adversarial_images\", g), exist_ok=True)\n",
        "\n",
        "        # Filter for AI-generated only if requested (Label=0)\n",
        "        if ai_only:\n",
        "            self.data = self.data[self.data[\"Label\"] == 0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        filepath = row[\"Filepath\"]\n",
        "        features = row[\"Features\"]\n",
        "        label = row[\"Label\"]\n",
        "\n",
        "        sample = {\"filepath\": filepath, \"features\": features, \"label\": label}\n",
        "\n",
        "        if self.transform is not None:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "id": "Iwcu2W0HyIHT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "def collate_for_pgd(batch):\n",
        "    filepaths = [item[\"filepath\"] for item in batch if os.path.exists(item[\"filepath\"])]\n",
        "\n",
        "    # read in images from filepaths using torchvision\n",
        "    images = [torchvision.io.read_image(filepath) for filepath in filepaths]\n",
        "    images = torch.stack(images)\n",
        "    images = processor(images=images, return_tensors=\"pt\")[\"pixel_values\"]\n",
        "\n",
        "    outpaths = [os.path.join(\"data\",\n",
        "                             \"adversarial_images\",\n",
        "                             filepath.split(os.sep)[-2],\n",
        "                             f\"adversarial_{os.path.basename(filepath)}.png\")\n",
        "                for filepath in filepaths]\n",
        "\n",
        "    return images, outpaths\n"
      ],
      "metadata": {
        "id": "acd9_39z0A4z"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ArtEmbeddingDataset(os.path.join(ROOT, \"CLIP_embeddings.csv\"), ai_only=True)\n",
        "dataloader = DataLoader(dataset, batch_size=32, collate_fn=collate_for_pgd, shuffle=False)"
      ],
      "metadata": {
        "id": "-IH9L9WcyJhM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare PGD Attack on Method"
      ],
      "metadata": {
        "id": "sERPstaYGzR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchattacks import PGD\n",
        "\n",
        "class CLIPPGDAttack(PGD):\n",
        "    def __init__(self, model, eps=8/255, alpha=2/255, steps=10, random_start=True):\n",
        "        super().__init__(model, eps, alpha, steps, random_start)\n",
        "\n",
        "    def get_logits(self, inputs):\n",
        "        if self._normalization_applied is False:\n",
        "            inputs = self.normalize(inputs)\n",
        "\n",
        "        # Get image features from the vision model\n",
        "        vision_outputs = self.model.vision_model(inputs)\n",
        "        image_features = vision_outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        return image_features"
      ],
      "metadata": {
        "id": "V7Bl-jYQDfN5"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = model.model\n",
        "processor = model.processor\n",
        "\n",
        "attack = CLIPPGDAttack(discriminator, eps=8/255, alpha=2/255, steps=10, random_start=True)\n",
        "\n",
        "\n",
        "for images, outpaths in tqdm(dataloader, desc=\"Adversarial Attacks\"):\n",
        "\n",
        "  if all(os.path.exists(outpath) for outpath in outpaths):\n",
        "    continue\n",
        "\n",
        "  images = images.to(discriminator.device)\n",
        "  target_label = torch.tensor([1]*len(outpaths)).to(discriminator.device)\n",
        "\n",
        "  # # Generate adversarial example\n",
        "  adversarial_image = attack(images, target_label)\n",
        "\n",
        "  for i in range(adversarial_image.shape[0]):\n",
        "    # Get the image and filepath for the current index\n",
        "    image = adversarial_image[i]\n",
        "    filepath = outpaths[i]\n",
        "\n",
        "    # Save the image using torchvision.utils.save_image\n",
        "    torchvision.utils.save_image(image, filepath)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESh1cYWYGAMZ",
        "outputId": "8df16e3b-50a7-4c3c-f899-a75b3eda4c2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Adversarial Attacks:   7%|▋         | 29/397 [01:17<18:16,  2.98s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find Descriminatory Prediction on Adversarial Images"
      ],
      "metadata": {
        "id": "kcZeWwAj-9gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load(os.path.join(ROOT, \"CLIP_discriminator.pt\"), weights_only=False)\n"
      ],
      "metadata": {
        "id": "C9ZcLiry2LFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def collate_for_svm(batch):\n",
        "    filepaths = [item[\"filepath\"] for item in batch if os.path.exists(item[\"filepath\"])]\n",
        "    outpaths = [os.path.join(\"data\",\n",
        "                             \"adversarial_images\",\n",
        "                             filepath.split(os.sep)[-2],\n",
        "                             f\"adversarial_{os.path.basename(filepath)}.png\")\n",
        "                for filepath in filepaths]\n",
        "\n",
        "    # read in images from filepaths using torchvision\n",
        "    images = [torchvision.io.read_image(filepath) for filepath in outpaths]\n",
        "    images = torch.stack(images)\n",
        "    embeddings = model.run_clip(images)\n",
        "    return embeddings, outpaths"
      ],
      "metadata": {
        "id": "uwTdTWq6_FrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_dataloader = DataLoader(dataset, batch_size=32, collate_fn=collate_for_svm, shuffle=False)"
      ],
      "metadata": {
        "id": "Vcibh9BETwVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "records = []\n",
        "\n",
        "\n",
        "for embeddings, paths in tqdm(svm_dataloader, \"Saving Adversarial Embeddings\"):\n",
        "  for i, emb in enumerate(embeddings):\n",
        "    records.append({\"filepath\": paths[i], \"embedding\": emb})\n",
        "\n",
        "df = pd.DataFrame(records)\n",
        "df.to_csv(os.path.join(ROOT, \"CLIP_adversarial_embeddings.csv\"), index=False)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "bvY4ovbFTzd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [0] * len(df)\n",
        "\n",
        "df_adv = pd.read_csv(os.path.join(ROOT, \"CLIP_adversarial_embeddings.csv\"))\n",
        "df_adv[\"embedding\"] = df_adv[\"embedding\"].apply(string_to_np)\n",
        "embeddings = df_adv[\"embedding\"].to_numpy()\n",
        "embeddings = np.vstack(embeddings)\n",
        "print(embeddings.shape)\n",
        "\n",
        "model.evaluate(embeddings, labels)"
      ],
      "metadata": {
        "id": "-cpTKToJpV8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wjic-cghsL4t"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}