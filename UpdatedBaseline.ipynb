{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline for CLIP AI Detection Adversarial Attack"
      ],
      "metadata": {
        "id": "zJENsdUGGHcM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "cwbcNfoEGUT6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7SwWf4G4ebZ",
        "outputId": "984c85c5-ff38-4340-bdbe-1b6cc2f2f595"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQq3Ey574gja",
        "outputId": "c834855e-7d1d-4e16-c819-763e8d5daa9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1SUnyLWY7LvpxPNxyFvip9Ae4S39ePRqJ/IDL Image Generation\n"
          ]
        }
      ],
      "source": [
        "%cd '/content/drive/MyDrive/IDL Image Generation'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "AziwBTP48KWo",
        "outputId": "16fd5ab4-008b-4101-9e73-d9756d0bd6a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m519.5/664.8 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting torchattacks\n",
            "  Downloading torchattacks-3.5.1-py3-none-any.whl.metadata (927 bytes)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.11/dist-packages (from torchattacks) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.11/dist-packages (from torchattacks) (0.21.0+cu124)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from torchattacks) (1.14.1)\n",
            "Requirement already satisfied: tqdm>=4.56.1 in /usr/local/lib/python3.11/dist-packages (from torchattacks) (4.67.1)\n",
            "Collecting requests~=2.25.1 (from torchattacks)\n",
            "  Downloading requests-2.25.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy>=1.19.4 in /usr/local/lib/python3.11/dist-packages (from torchattacks) (2.0.2)\n",
            "Collecting chardet<5,>=3.0.2 (from requests~=2.25.1->torchattacks)\n",
            "  Downloading chardet-4.0.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting idna<3,>=2.5 (from requests~=2.25.1->torchattacks)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting urllib3<1.27,>=1.21.1 (from requests~=2.25.1->torchattacks)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests~=2.25.1->torchattacks) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.7.1->torchattacks)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.7.1->torchattacks)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.7.1->torchattacks)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.7.1->torchattacks)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.7.1->torchattacks)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.7.1->torchattacks)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.7.1->torchattacks)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.7.1->torchattacks)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.7.1->torchattacks)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.7.1->torchattacks)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.7.1->torchattacks) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision>=0.8.2->torchattacks) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.7.1->torchattacks) (3.0.2)\n",
            "Downloading torchattacks-3.5.1-py3-none-any.whl (142 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.0/142.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: urllib3, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, idna, chardet, requests, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchattacks\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.3.0\n",
            "    Uninstalling urllib3-2.3.0:\n",
            "      Successfully uninstalled urllib3-2.3.0\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.25.1 which is incompatible.\n",
            "bigframes 1.42.0 requires requests>=2.27.1, but you have requests 2.25.1 which is incompatible.\n",
            "sphinx 8.2.3 requires requests>=2.30.0, but you have requests 2.25.1 which is incompatible.\n",
            "tweepy 4.15.0 requires requests<3,>=2.27.0, but you have requests 2.25.1 which is incompatible.\n",
            "yfinance 0.2.55 requires requests>=2.31, but you have requests 2.25.1 which is incompatible.\n",
            "google-genai 1.10.0 requires requests<3.0.0,>=2.28.1, but you have requests 2.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-4.0.0 idna-2.10 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 requests-2.25.1 torchattacks-3.5.1 urllib3-1.26.20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "chardet",
                  "idna",
                  "nvidia",
                  "requests",
                  "urllib3"
                ]
              },
              "id": "eaf218f2f82b4424b7458ede10586dd3"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install transformers torch --quiet\n",
        "!pip install torchattacks --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zMkDJ-xI4iJj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import ast\n",
        "import csv\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
        "\n",
        "from torchattacks import PGD\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FJk4lpvt3PX6"
      },
      "outputs": [],
      "source": [
        "DIFFUSION_MODELS = [\"openjourney\", \"titan\", \"dalle\", \"real\", \"openjourney_v4\"]\n",
        "ROOT = '/content/drive/MyDrive/IDL Image Generation'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process Images with CLIP"
      ],
      "metadata": {
        "id": "89x0uZAPGb_w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gV80yynV3KUq"
      },
      "outputs": [],
      "source": [
        "def collect_images(generators):\n",
        "  generated_images = {}\n",
        "  for directory in generators:\n",
        "    for filepath in os.listdir(os.path.join(ROOT, \"data\", directory)):\n",
        "      if filepath.lower().endswith(\".jpg\"):\n",
        "        full_path = os.path.join(ROOT, \"data\", directory, filepath)\n",
        "        id_idx = filepath.rfind('_') + 1\n",
        "        id = filepath[id_idx:-4]\n",
        "        label = 1 if directory == \"real\" else 0\n",
        "        generated_images[full_path] = {\n",
        "            \"generator\": directory,\n",
        "            \"label\": label, # 0 = fake, 1 = real\n",
        "            \"id\": id,\n",
        "        }\n",
        "  return generated_images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "VeJb_dwf4ZBY"
      },
      "outputs": [],
      "source": [
        "images = collect_images(DIFFUSION_MODELS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fkDSUmX6yQC",
        "outputId": "c47bc30e-23f1-4d45-f1c9-eec155b218cf",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CLIPModel(\n",
              "  (text_model): CLIPTextTransformer(\n",
              "    (embeddings): CLIPTextEmbeddings(\n",
              "      (token_embedding): Embedding(49408, 512)\n",
              "      (position_embedding): Embedding(77, 512)\n",
              "    )\n",
              "    (encoder): CLIPEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x CLIPEncoderLayer(\n",
              "          (self_attn): CLIPSdpaAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): QuickGELUActivation()\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (vision_model): CLIPVisionTransformer(\n",
              "    (embeddings): CLIPVisionEmbeddings(\n",
              "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
              "      (position_embedding): Embedding(50, 768)\n",
              "    )\n",
              "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (encoder): CLIPEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x CLIPEncoderLayer(\n",
              "          (self_attn): CLIPSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): QuickGELUActivation()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
              "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "print(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7lTfd42V60Iv"
      },
      "outputs": [],
      "source": [
        "def run_clip(image_path):\n",
        "  with Image.open(image_path) as img:\n",
        "    img = img.convert(\"RGB\")\n",
        "    if img.size != (512, 512):\n",
        "      print(f\"Resizing {image_path} to 512x512\")\n",
        "      img = img.resize((512, 512))\n",
        "      img.save(image_path)\n",
        "  # inputs = processor(images=img, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "  inputs = processor(images=img, return_tensors=\"pt\")\n",
        "  inputs = inputs.to(device)\n",
        "  with torch.no_grad():\n",
        "    # image_features = model.get_image_features(**inputs)\n",
        "    outputs = model.vision_model(inputs.pixel_values)\n",
        "    image_features = outputs.last_hidden_state[:, 0, :]\n",
        "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "    return image_features.squeeze().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CyXYhHNvdniX"
      },
      "outputs": [],
      "source": [
        "def get_processed_filepaths(csv_path: str):\n",
        "    processed_filepaths = set()\n",
        "    if not os.path.isfile(csv_path):\n",
        "        return set()\n",
        "    try:\n",
        "        with open(csv_path, 'r', newline='') as csvfile:\n",
        "            reader = csv.reader(csvfile)\n",
        "            next(reader, None)  # Skip the header row\n",
        "            for row in reader:\n",
        "                if row:  # Check if the row is not empty\n",
        "                    processed_filepaths.add(row[0])  # Add the filepath (first column) to the set\n",
        "    except FileNotFoundError:\n",
        "        print(f\"CSV file not found: {csv_path}\")\n",
        "    except csv.Error as e:\n",
        "        print(f\"Error reading CSV file: {e}\")\n",
        "\n",
        "    return processed_filepaths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mtO0L5VdWaG0"
      },
      "outputs": [],
      "source": [
        "def process_images_and_log(image_dict: dict, csv_fname: str) -> None:\n",
        "    processed_filepaths = get_processed_filepaths(os.path.join(ROOT, csv_fname))\n",
        "    print(f\"{len(processed_filepaths)} files have already been processed\")\n",
        "    for path, info in image_dict.items():\n",
        "      if path in processed_filepaths:\n",
        "        # print(f\"{info['generator']}_{info['id']} has already been processed\")\n",
        "        continue\n",
        "\n",
        "      encoding = run_clip(path)\n",
        "\n",
        "      data = [path, info[\"id\"], info[\"generator\"], str(encoding), info[\"label\"]]\n",
        "\n",
        "      csv_path = os.path.join(ROOT, csv_fname)\n",
        "      file_exists = os.path.isfile(csv_path)\n",
        "      with open(csv_path, 'a', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        if not file_exists:\n",
        "            writer.writerow([\"Filepath\", \"Id\", \"Generator\", \"Features\", \"Label\"])\n",
        "        writer.writerow(data)\n",
        "      print(f\"{info['generator']}_{info['id']} encoded and logged to csv\")\n",
        "    print(f\"All input images encoded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "i2P7Gy2fZ_Hn",
        "outputId": "03cb70e5-3b8e-4b07-84f2-7aaa12dd482a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16312 files have already been processed\n",
            "All input images encoded!\n"
          ]
        }
      ],
      "source": [
        "process_images_and_log(images, \"images_hidden_state_embedding.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train SVM for AI Image Detection"
      ],
      "metadata": {
        "id": "veHjUmu6GfZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPSVMDiscriminator:\n",
        "    def __init__(self, model_name=\"openai/clip-vit-base-patch32\", device=None):\n",
        "        self.device = device if device is not None else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(\"Running on:\", self.device)\n",
        "        self.model = CLIPModel.from_pretrained(model_name)\n",
        "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        self.svm = SVC(kernel=\"linear\", C=1.0, probability=True)\n",
        "        self.svm_trained = False\n",
        "\n",
        "    def run_clip(self, image_path):\n",
        "        with Image.open(image_path) as img:\n",
        "            img = img.convert(\"RGB\")\n",
        "            if img.size != (512, 512):\n",
        "                img = img.resize((512, 512))\n",
        "                img.save(image_path)\n",
        "        inputs = self.processor(images=img, return_tensors=\"pt\").to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.vision_model(inputs.pixel_values)\n",
        "            image_features = outputs.last_hidden_state[:, 0, :]\n",
        "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "            return image_features.squeeze().cpu().numpy()\n",
        "\n",
        "    def train_svm(self, X_train, y_train):\n",
        "        self.svm.fit(X_train, y_train)\n",
        "        self.svm_trained = True\n",
        "        train_accuracy = self.svm.score(X_train, y_train)\n",
        "        print(f\"Training accuracy for discriminator: {train_accuracy:.4f}\")\n",
        "        return self.svm\n",
        "\n",
        "    def predict_from_embeddings(self, embeddings):\n",
        "        preds = self.svm.predict(embeddings)\n",
        "        probs = self.svm.predict_proba(embeddings)[:, 1]\n",
        "        return preds, probs\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        model = self.svm\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='weighted')\n",
        "        recall = recall_score(y_test, y_pred, average='weighted')\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "        auc = roc_auc_score(y_test, y_pred_proba)\n",
        "        ap_per_class = []\n",
        "        for class_label in np.unique(y_test):\n",
        "            y_test_binary = (y_test == class_label).astype(int)\n",
        "            ap = average_precision_score(y_test_binary, y_pred_proba)\n",
        "            ap_per_class.append(ap)\n",
        "        map_score = np.mean(ap_per_class)\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1 Score: {f1:.4f}\")\n",
        "        print(f\"AUC: {auc:.4f}\")\n",
        "        print(f\"mAP: {map_score:.4f}\")\n",
        "        return {\n",
        "            \"accuracy\": accuracy,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1,\n",
        "            \"auc\": auc,\n",
        "            \"map\": map_score\n",
        "        }"
      ],
      "metadata": {
        "id": "A58hsAQWHKMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "iG4mssemsXyx"
      },
      "outputs": [],
      "source": [
        "def train(X_train, y_train):\n",
        "    # Initialize and train the SVM classifier\n",
        "    svm_classifier = SVC(kernel='linear', C=1.0, probability=True)\n",
        "    svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate the classifier\n",
        "    train_accuracy = svm_classifier.score(X_train, y_train)\n",
        "\n",
        "    print(f\"Training accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "    return svm_classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HcFuDsX7-df7"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, X_test, y_test):\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "    # Calculate mean average precision (mAP)\n",
        "    ap_per_class = []\n",
        "    for class_label in np.unique(y_test):\n",
        "        y_test_binary = (y_test == class_label).astype(int)\n",
        "        ap = average_precision_score(y_test_binary, y_pred_proba)\n",
        "        ap_per_class.append(ap)\n",
        "    map_score = np.mean(ap_per_class)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"AUC: {auc:.4f}\")\n",
        "    print(f\"mAP: {map_score:.4f}\")\n",
        "\n",
        "    # Return metrics as a dictionary\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "        \"auc\": auc,\n",
        "        \"map\": map_score\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": true,
        "id": "dIqB_Cu49nH4"
      },
      "outputs": [],
      "source": [
        "def string_to_np(feature_str):\n",
        "    embedding_list = np.fromstring(feature_str[1:-1], sep=' ')\n",
        "    assert len(embedding_list) == 768\n",
        "    return np.array(embedding_list)\n",
        "\n",
        "\n",
        "df = pd.read_csv(os.path.join(ROOT, \"images_hidden_state_embedding.csv\"), usecols=range(5))\n",
        "X = np.stack(df[\"Features\"].apply(string_to_np).to_numpy())\n",
        "y = df[\"Label\"].to_numpy()\n",
        "\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsDsi5yHJhCg",
        "outputId": "fe7b8d81-bdcf-446a-bffe-6a01f631c5f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16312, 768) (16312,)\n"
          ]
        }
      ],
      "source": [
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEJb3ouW92sT"
      },
      "outputs": [],
      "source": [
        "model = CLIPSVMDiscriminator()\n",
        "\n",
        "\n",
        "svm = model.train(X_train, y_train)\n",
        "print(\"Testing Metrics:\")\n",
        "metrics = model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate method on Unseen Generators"
      ],
      "metadata": {
        "id": "N6h1evhBGlfE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rqm9mk25BnBA",
        "outputId": "60f66d43-4efe-46b6-be56-de28dc71db41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results for no-openjourney_v4 model\n",
            "Training accuracy: 0.9978\n",
            "Results on openjourney_v4:\n",
            "Accuracy: 0.9968\n",
            "Precision: 1.0000\n",
            "Recall: 0.9968\n",
            "F1 Score: 0.9984\n",
            "AUC: nan\n",
            "mAP: 1.0000\n",
            "******************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "for generator in [\"openjourney\", \"titan\", \"dalle\", \"openjourney_v4\"]:\n",
        "  if generator == \"real\":\n",
        "    continue\n",
        "\n",
        "  test_df = df[df['Generator'] == generator]\n",
        "  train_df = df[df['Generator'] != generator]\n",
        "\n",
        "  # Reset the index for both partitions\n",
        "  test_df = test_df.reset_index(drop=True)\n",
        "  train_df = train_df.reset_index(drop=True)\n",
        "\n",
        "  X_test = np.stack(test_df[\"Features\"].apply(string_to_np).to_numpy())\n",
        "  y_test = test_df[\"Label\"].to_numpy()\n",
        "\n",
        "  X_train = np.stack(train_df[\"Features\"].apply(string_to_np).to_numpy())\n",
        "  y_train = train_df[\"Label\"].to_numpy()\n",
        "\n",
        "  print(f\"Results for no-{generator} model\")\n",
        "  svm = train(X_train, y_train)\n",
        "  print(f\"Results on {generator}:\")\n",
        "  metrics = evaluate(svm, X_test, y_test)\n",
        "  print(\"******************************************\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare PGD Attack on Method"
      ],
      "metadata": {
        "id": "sERPstaYGzR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchattacks import PGD\n",
        "\n",
        "class CLIPPGDAttack(PGD):\n",
        "    def __init__(self, model, eps=8/255, alpha=2/255, steps=10, random_start=True):\n",
        "        super().__init__(model, eps, alpha, steps, random_start)\n",
        "\n",
        "    def get_logits(self, inputs):\n",
        "        if self._normalization_applied is False:\n",
        "            inputs = self.normalize(inputs)\n",
        "\n",
        "        # Get image features from the vision model\n",
        "        vision_outputs = self.model.vision_model(inputs)\n",
        "        image_features = vision_outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        return image_features"
      ],
      "metadata": {
        "id": "V7Bl-jYQDfN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ESh1cYWYGAMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZn1dczaY3fM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}